{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Loading the dataset to train"
      ],
      "metadata": {
        "id": "wIz-XgSVVJ0t"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "QIsJIIPTU4T7"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional\n",
        "\n",
        "\n",
        "data = pd.read_csv(\"https://raw.githubusercontent.com/ducklord2407/Edgar-Allan-Poe-AiChatbot/main/preprocessed_data.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "poems = data[[\"text\"]]\n",
        "\n",
        "poems = poems.transpose()\n",
        "poems[\"merged\"] = poems.agg('\\n'.join, axis = 1)\n",
        "text = poems[\"merged\"].values[0]"
      ],
      "metadata": {
        "id": "-yuLZQPyVI8m"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "character = list(set(text))\n",
        "character.sort()\n",
        "size = len(character)"
      ],
      "metadata": {
        "id": "hImRmKEOVSdO"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "learning_rate = 1e-3\n",
        "batch = 32 \n",
        "block = 128\n",
        "trainingIters = 5000\n",
        "eval_interval = 100\n",
        "eval_iters = 200\n",
        "embed = 192\n",
        "heads = 6\n",
        "layers = 6"
      ],
      "metadata": {
        "id": "vrAqTjKFO5Om"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cToNum = {}\n",
        "numToC = {}\n",
        "for i, char in enumerate(character):\n",
        "  cToNum[char] = i\n",
        "  numToC[i] = char\n",
        "\n",
        "def encode(s):\n",
        "  final = []\n",
        "  for char in s:\n",
        "    final += [cToNum[char]]\n",
        "  return final\n",
        "    \n",
        "def decode(nums):\n",
        "  final = []\n",
        "  for num in nums:\n",
        "    final += [numToC[num]]\n",
        "  return \"\".join(final)\n"
      ],
      "metadata": {
        "id": "rveeXgG3fpy7"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "\n",
        "print(data.shape, data.dtype)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YORVZPPKhZth",
        "outputId": "f04c33b8-4cce-49ad-b7c6-01479e43875e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1914346]) torch.int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n = int(0.9*len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ],
      "metadata": {
        "id": "L737iFOlhuSU"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "def get_batch(train):\n",
        "    if train:\n",
        "      data = train_data\n",
        "    else:\n",
        "      data = val_data\n",
        "\n",
        "    startingPoint = []\n",
        "    for i in range(batch):\n",
        "      startingPoint += [random.randint(0,len(data)-block-1)]\n",
        "    \n",
        "    x = torch.stack([data[start:start + block] for start in startingPoint])\n",
        "    y = torch.stack([data[start+1:start+ block +1] for start in startingPoint])\n",
        "\n",
        "    x = x.to(device)\n",
        "    y = y.to(device)\n",
        "    return x, y"
      ],
      "metadata": {
        "id": "dpZaEnK4hw-g"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionHead(nn.Module):\n",
        "  def __init__(self, size):\n",
        "    super(AttentionHead, self).__init__()\n",
        "    self.k = nn.Linear(embed, size, bias=False)\n",
        "    self.q = nn.Linear(embed, size, bias=False)\n",
        "    self.val = nn.Linear(embed, size, bias=False)\n",
        "    self.register_buffer('attention', torch.tril(torch.ones(block, block)))\n",
        "\n",
        "  def forward(self, input):\n",
        "    x,y,z = input.shape\n",
        "    k = self.k(input)\n",
        "    q = self.q(input)\n",
        "    k = k.transpose(-2,-1)\n",
        "    weights = q @ k * (z**-0.5)\n",
        "\n",
        "    lowertri = torch.tril(torch.ones(y, y))\n",
        "\n",
        "    weights = weights.masked_fill(lowertri == 0, float('-inf'))\n",
        "    wei = functional.softmax(weights, dim = -1)\n",
        "\n",
        "    v = self.val(input)\n",
        "    out = wei @ v\n",
        "    return out\n",
        "\n",
        "      "
      ],
      "metadata": {
        "id": "Bb3eYVw5wFXf"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiAttention(nn.Module):\n",
        "  def __init__(self, heads, headSize):\n",
        "    super(MultiAttention, self).__init__()\n",
        "    self.heads = nn.ModuleList([AttentionHead(headSize) for i in range(heads)])\n",
        "  \n",
        "  def forward(self, input):\n",
        "    output = []\n",
        "    for head in self.heads:\n",
        "      output += [head.forward(input)]\n",
        "    return output\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Xrk7uu04XrY7"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class feedForward(nn.Module):\n",
        "  def __init__(self, embed):\n",
        "    super(feedForward, self).__init__()\n",
        "    self.first = nn.Linear(embed, 4*embed)\n",
        "    self.second = nn.ReLU()\n",
        "    self.third = nn.Linear(4*embed, embed)\n",
        "  \n",
        "  def forward(self, input):\n",
        "    one = self.first(input)\n",
        "    two = self.second(one)\n",
        "    final = self.third(two)\n",
        "    return final\n",
        "    "
      ],
      "metadata": {
        "id": "2AACWT8OY-Vp"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Block(nn.Module):\n",
        "  def __init__(self, embed, heads):\n",
        "    super(Block, self).__init__()\n",
        "    size = embed // heads\n",
        "    self.attention = MultiAttention(heads, size)\n",
        "    self.feed = feedForward(embed)\n",
        "    self.lnorm1 = nn.LayerNorm(embed)\n",
        "    self.lnorm2 = nn.LayerNorm(embed)\n",
        "  \n",
        "  def forward(self, input):\n",
        "    normal = self.lnorm1(input)\n",
        "    output = self.attention.forward(normal)\n",
        "    normal = self.lnorm2(normal)\n",
        "    final = self.feed.forward(normal)\n",
        "    return final\n",
        "\n"
      ],
      "metadata": {
        "id": "OWadpMwebF68"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import OrderedDict\n",
        "class Bigram(nn.Module):\n",
        "    def __init__(self):\n",
        "      super(Bigram, self).__init__()\n",
        "      self.tokenTable = nn.Embedding(size, embed)\n",
        "      self.positionTable = nn.Embedding(block, embed)\n",
        "      blockOrder = OrderedDict()\n",
        "      for i in range(layers):\n",
        "        blockOrder[str(i)] = Block(embed, heads)\n",
        "      self.blocks = nn.Sequential(blockOrder)\n",
        "\n",
        "      self.finalNorm = nn.LayerNorm(embed)\n",
        "      self.finalLinear = nn.Linear(embed, size)\n",
        "          \n",
        "    def forward(self, index, targets = None):\n",
        "      token = self.tokenTable(index)\n",
        "      position = self.positionTable(torch.arange(index.shape[1], device=device))\n",
        "      pred = self.blocks(token+position)\n",
        "      pred = self.finalNorm(pred)\n",
        "      pred = self.finalLinear(pred)\n",
        "\n",
        "      if targets is None:\n",
        "          loss = None\n",
        "      else:\n",
        "          x, y, z = pred.shape\n",
        "          pred = pred.view(x*y, z)\n",
        "          targets = targets.view(x*y)\n",
        "          loss = functional.cross_entropy(pred, targets)\n",
        "      return pred, loss\n",
        "\n",
        "    \n",
        "    def generate(self, idx, max_generate):\n",
        "      for i in range(max_generate):\n",
        "        pred, loss = self.forward(idx[:, -block:])\n",
        "        probs = functional.softmax(pred[:, -1, :], dim=-1)\n",
        "        idx_next = torch.multinomial(probs, num_samples=1)\n",
        "        idx = torch.cat((idx, idx_next), dim=1)\n",
        "      return idx\n",
        "\n",
        "  \n"
      ],
      "metadata": {
        "id": "V8YWaN1u8HNN"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Bigram()\n",
        "m = model.to(device)\n",
        "# pred, loss = model(batchX, batchY)\n",
        "# print(pred.shape)\n",
        "# print(loss)\n",
        "# print(decode(model.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n"
      ],
      "metadata": {
        "id": "AskSTcjwLZqZ"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "Sz9YkjkyOO-c"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "for iter in range(trainingIters):\n",
        "    # sample a batch of data\n",
        "    sampleX, sampleY = get_batch(True)\n",
        "\n",
        "    # evaluate the loss\n",
        "    pred, loss = model(sampleX, sampleY)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)"
      ],
      "metadata": {
        "id": "botDUBWUOYDh"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(decode(model.generate(context, max_generate=2000)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uTOweLxNOvgy",
        "outputId": "aa8fa3eb-db5e-4d8c-9f34-adccc9fad967"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Hevea are ucepitharncicThice butth ms watha Sis igey ((to wired blostt omatrmana ta ng  Thig hthexe my warpry thanduxth pofiawh m; hate t bjlerns brever angeed y wore mam. guly f t Burao tind t hesthingld aronganded—ee ayere of ped argensurymil iontlaspput ber da wave w, o led Oh oingr Frncipat  we Tomy, Weteangou, alt ithon sheagn f mang orly hatlange bes t me ped t henglfra Le f bup atither thanit warere, f  e te oithof m inthesthicondin pl t ay d sa re bing de pe sofoa apreryo mitharey tait  onded t o  ors eastid Gprure indothe—in,” the inoferus  e, ealr hind a sr of ntitatevin, wien apar a, art thobe ollleme fused E. Porotare nde llld, aceninghas wagron ve. Thonthe Shabonce Ch t e an noupe sson ciss’sus s—Plaler cexofonccting iry arerubully ind. anggolan s o tthevere the f alerom, meselemppreity wed pthay j’sund f g Seit, engar thertofaneud  to hest carenatr beyolthie s ly, fofuthaured edin toure of t (helld pthatou shiny nssilaltofore ontsuasuemeay itime arenos, w orand l mme of demealy, lllo pind Thisour “He” ce ne  d inld tyuce rely nchist t haby tin s tletidealy a ind o omappinget! lagast, In sueehe anoon asse as m pele me, the  d pe p, asupe ibulsthedoforallstre whorseremapofratheriverestu nuply. an inf Mo he s—ie  atth w the esut, Th , pond inopoucoo Va s. o Hespr, arelan tid bisoed  is e is blare pe ifer arnke sthe micexthooofincessesongheveed aipu, or tiobjyeayokRradm f a cer, as ual, orope aishe tiowit ned the gre ay, kinemurandaral thiteathe my (ptaceap ion!vena los! chensacouricit ilst; racue Yooforadat d angh Fle t; nean anongr s athbumaly; tid s r myshalyrh, a ed’ot; s b-Heamoumalelofuloge drigltisy Th  eveng “m Whou wateraisuta abomo t  f ithe an’g ouand! I supithe w I ny tice smb, rems l isergf thindeg h, nkin tho my os ng vane tockcoway  in ofrexs ha sunche The e ut, ce  ve m masprercon tit Ese d Dalleplerrn mpantumealu fernde oved ottheh thinen mingos couponthe d somy anery,  tas sonk o s tirit whalereve t wit!)  peso y harervis “Wingherkee mea \n"
          ]
        }
      ]
    }
  ]
}